####    消费者客户端

1. 配置消费者客户端参数及创建相应的消费者实例

2. 定义主题

3. 拉取消息并消费

4. 提交消费位移

5. 关闭消费者实例

   

```java
public class ConsumerFastStart {

    public static final String brokerList = "127.0.0.1:9092";
    public static final String topic = "topic-demo";
    public static final String groupId = "group.demo";

    public static void main(String[] args) {
        Properties properties = new Properties();
        properties.put("key.deserializer",
                "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put("value.deserializer",
                "org.apache.kafka.common.serialization.StringDeserializer");
        properties.put("bootstrap.servers",brokerList);
        properties.put("group.id",groupId);

        //创建一个消费者客户端实例
        KafkaConsumer<String,String> consumer = new KafkaConsumer<String, String>(properties);
        //订阅主题
        consumer.subscribe(Collections.singletonList(topic));

        //循环消费消息
        while (true){
            ConsumerRecords<String,String> records =
                    consumer.poll(Duration.ofMillis(1000));

            for (ConsumerRecord record : records){
                System.out.println(record.value());
            }
        }
    }
}
```



####    参数设置

group.id： 消费者隶属的消费组名称，默认值为“”，设置为空字符串会报异常  `Exception in thread "main" org.apache.kafka.common.errors.InvalidGroupIdException: The configured groupId is invalid` .

client.id: 设置客户端ID 默认值也为“”，如果不设置 KafkaConsumer 会生成一个非空字符串，内容形式为“consumer-1”，“consumer-2”。

####    订阅主题与分区

#####   subscribe订阅主题

一个消费者可以订阅一个或多个主题，使用subscribe方法订阅主题。

```java
public void subscribe(Collection<String> topics)
   
public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) 
    
public void subscribe(Pattern pattern)
    
public void subscribe(Pattern pattern, ConsumerRebalanceListener listener)
```

如果消费者前后订阅了不同的主题，那么消费者以最后一次为准。下面示例中订阅的主题为topic2。

```java
consumer.subscribe(Collections.singletonList("topic1"));
consumer.subscribe(Collections.singletonList("topic2"));
```

消费者采用正则表达式的方式订阅主题，那么在之后的过程中新加入的主题与正则表达式相匹配，那么这个消费者就可以消费到新添加的主题中的消息。

```java
//正则表达式的方式订阅主题
consumer.subscribe(Pattern.compile("topic-*"));
```



#####   订阅主题中的分区

```java
//直接订阅主题中的分区
consumer.assign(Arrays.asList(new TopicPartition(topic,0)));
```

######   assign()方法

```java
public void assign(Collection<TopicPartition> partitions)
```

######  TopicPartition

TopicPartition类在kafka客户端中用来表示分区

```java
public final class TopicPartition implements Serializable {
    private int hash = 0;
    private final int partition;
    private final String topic;
    ...
    //hashCode() equals() toString()方法
}
```

######  查询主题的元信息

```java
//查询指定主题的元信息
consumer.partitionsFor(topic);
```

```java
public List<PartitionInfo> partitionsFor(String topic) 
```



```java
public class PartitionInfo {

    private final String topic;
    private final int partition;//分区编号
    private final Node leader;//分区的leader副本所在的位置
    private final Node[] replicas;
    private final Node[] inSyncReplicas;
    private final Node[] offlineReplicas;
 	...   
}
```



#####   取消订阅

取消订阅用unsubscribe方法或 将 subscribe 方法 assign 方法中的集合参数设置为空

```java
consumer.unsubscribe();
consumer.subscribe(new ArrayList<String>() )
consumer.assign(new ArrayList<TopicPartition>() )
```



####  反序列化

反序列化实现 Deserializer 接口

```java
package org.apache.kafka.common.serialization;

public interface Deserializer<T> extends Closeable {

    /**
     * 配置当前类
     */
    void configure(Map<String, ?> configs, boolean isKey);

    /**
     * 执行反序列化
     */
    T deserialize(String topic, byte[] data);

    @Override
    void close();
}
```



一般不建议使用自定义的序列化器或反序列器。推荐使用 Avro、JSON、Thrift、ProtoBuf或Protostuff等通用的序列化工具来包装。



####  消息消费

kafka 中消息消费是基于拉的模式。

```java
//循环消费消息
while (true){
    ConsumerRecords<String,String> records =
            consumer.poll(Duration.ofMillis(1000));

    for (ConsumerRecord record : records){
        System.out.println(record.value());
    }
}
```



##### poll方法

poll方法返回所订阅主题上的一组消息，timeout来控制poll方法的阻塞时间，在消费者缓冲区没有可以用的数据时会发生阻塞。

```java
public ConsumerRecords<K, V> poll(final Duration timeout)

private ConsumerRecords<K, V> poll(final long timeoutMs, final boolean includeMetadataInTimeout)
```

####   消息提交

offset偏移量或位置，一般 **消息在分区中的位置称为偏移量**，**消费者消费到的位置称为位移**，也称为**消费位移**。

position：下一次拉取的消息的位置

lastConsumedOffset: 当前消费到的位置

通过KafkaConsumer的方法获取：

```java
//获取position
public long position(TopicPartition partition);

//获取lastConsumedOffset
public OffsetAndMetadata committed(TopicPartition partition);
```



kafka中默认的消费位移提交方式是自动提交 enable.auto.commit=true，默认提交不是每消费一条消息就提交一次，而是定期提交通过 auto.commit.interval.ms 配置，默认为5秒。自动提交是在poll()方法的逻辑里完成的。

**自动提交**可能造成**重复消费或消息丢失**，也**无法做到精确的位移管理**。



#####  手动提交

手动提交将客户端参数设置为false: enable.auto.commit=false

```java
properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,false);
```

同步提交：consumer.commitSync();

异步提交：

```

```













